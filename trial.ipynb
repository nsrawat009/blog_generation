{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline, TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_data: {'Meta Data': {'1. Information': 'Daily Prices (open, high, low, close) and Volumes', '2. Symbol': 'AAPL', '3. Last Refreshed': '2024-05-31', '4. Output Size': 'Compact', '5. Time Zone': 'US/Eastern'}, 'Time Series (Daily)': {'2024-05-31': {'1. open': '191.4400', '2. high': '192.5700', '3. low': '189.9100', '4. close': '192.2500', '5. volume': '75158277'}, '2024-05-30': {'1. open': '190.7600', '2. high': '192.1800', '3. low': '190.6300', '4. close': '191.2900', '5. volume': '49947941'}, '2024-05-29': {'1. open': '189.6100', '2. high': '192.2470', '3. low': '189.5100', '4. close': '190.2900', '5. volume': '53068016'}, '2024-05-28': {'1. open': '191.5100', '2. high': '193.0000', '3. low': '189.1000', '4. close': '189.9900', '5. volume': '52280051'}, '2024-05-24': {'1. open': '188.8200', '2. high': '190.5800', '3. low': '188.0404', '4. close': '189.9800', '5. volume': '36326975'}, '2024-05-23': {'1. open': '190.9800', '2. high': '191.0000', '3. low': '186.6250', '4. close': '186.8800', '5. volume': '51005924'}, '2024-05-22': {'1. open': '192.2650', '2. high': '192.8231', '3. low': '190.2700', '4. close': '190.9000', '5. volume': '34648547'}, '2024-05-21': {'1. open': '191.0900', '2. high': '192.7300', '3. low': '190.9201', '4. close': '192.3500', '5. volume': '42309401'}, '2024-05-20': {'1. open': '189.3250', '2. high': '191.9199', '3. low': '189.0100', '4. close': '191.0400', '5. volume': '44361275'}, '2024-05-17': {'1. open': '189.5100', '2. high': '190.8100', '3. low': '189.1800', '4. close': '189.8700', '5. volume': '41282925'}, '2024-05-16': {'1. open': '190.4700', '2. high': '191.0950', '3. low': '189.6601', '4. close': '189.8400', '5. volume': '52845230'}, '2024-05-15': {'1. open': '187.9100', '2. high': '190.6500', '3. low': '187.3700', '4. close': '189.7200', '5. volume': '70399988'}, '2024-05-14': {'1. open': '187.5100', '2. high': '188.3000', '3. low': '186.2900', '4. close': '187.4300', '5. volume': '52393619'}, '2024-05-13': {'1. open': '185.4350', '2. high': '187.1000', '3. low': '184.6200', '4. close': '186.2800', '5. volume': '72044809'}, '2024-05-10': {'1. open': '184.9000', '2. high': '185.0900', '3. low': '182.1300', '4. close': '183.0500', '5. volume': '50759496'}, '2024-05-09': {'1. open': '182.5600', '2. high': '184.6600', '3. low': '182.1100', '4. close': '184.5700', '5. volume': '48982972'}, '2024-05-08': {'1. open': '182.8500', '2. high': '183.0700', '3. low': '181.4500', '4. close': '182.7400', '5. volume': '45057087'}, '2024-05-07': {'1. open': '183.4500', '2. high': '184.9000', '3. low': '181.3200', '4. close': '182.4000', '5. volume': '77305771'}, '2024-05-06': {'1. open': '182.3540', '2. high': '184.2000', '3. low': '180.4200', '4. close': '181.7100', '5. volume': '78569667'}, '2024-05-03': {'1. open': '186.6450', '2. high': '187.0000', '3. low': '182.6600', '4. close': '183.3800', '5. volume': '163224109'}, '2024-05-02': {'1. open': '172.5100', '2. high': '173.4150', '3. low': '170.8900', '4. close': '173.0300', '5. volume': '94214915'}, '2024-05-01': {'1. open': '169.5800', '2. high': '172.7050', '3. low': '169.1100', '4. close': '169.3000', '5. volume': '50383147'}, '2024-04-30': {'1. open': '173.3300', '2. high': '174.9900', '3. low': '170.0000', '4. close': '170.3300', '5. volume': '65934776'}, '2024-04-29': {'1. open': '173.3700', '2. high': '176.0300', '3. low': '173.1000', '4. close': '173.5000', '5. volume': '68169419'}, '2024-04-26': {'1. open': '169.8800', '2. high': '171.3400', '3. low': '169.1800', '4. close': '169.3000', '5. volume': '44838354'}, '2024-04-25': {'1. open': '169.5250', '2. high': '170.6100', '3. low': '168.1511', '4. close': '169.8900', '5. volume': '50558329'}, '2024-04-24': {'1. open': '166.5400', '2. high': '169.3000', '3. low': '166.2100', '4. close': '169.0200', '5. volume': '48251835'}, '2024-04-23': {'1. open': '165.3500', '2. high': '167.0500', '3. low': '164.9200', '4. close': '166.9000', '5. volume': '49537761'}, '2024-04-22': {'1. open': '165.5150', '2. high': '167.2600', '3. low': '164.7700', '4. close': '165.8400', '5. volume': '48116443'}, '2024-04-19': {'1. open': '166.2100', '2. high': '166.4000', '3. low': '164.0750', '4. close': '165.0000', '5. volume': '68149377'}, '2024-04-18': {'1. open': '168.0300', '2. high': '168.6400', '3. low': '166.5500', '4. close': '167.0400', '5. volume': '43122903'}, '2024-04-17': {'1. open': '169.6100', '2. high': '170.6500', '3. low': '168.0000', '4. close': '168.0000', '5. volume': '50901210'}, '2024-04-16': {'1. open': '171.7500', '2. high': '173.7600', '3. low': '168.2700', '4. close': '169.3800', '5. volume': '73711235'}, '2024-04-15': {'1. open': '175.3600', '2. high': '176.6300', '3. low': '172.5000', '4. close': '172.6900', '5. volume': '73531773'}, '2024-04-12': {'1. open': '174.2600', '2. high': '178.3600', '3. low': '174.2100', '4. close': '176.5500', '5. volume': '101670886'}, '2024-04-11': {'1. open': '168.3400', '2. high': '175.4600', '3. low': '168.1600', '4. close': '175.0400', '5. volume': '91070275'}, '2024-04-10': {'1. open': '168.8000', '2. high': '169.0900', '3. low': '167.1100', '4. close': '167.7800', '5. volume': '49709336'}, '2024-04-09': {'1. open': '168.7000', '2. high': '170.0800', '3. low': '168.3500', '4. close': '169.6700', '5. volume': '42231444'}, '2024-04-08': {'1. open': '169.0300', '2. high': '169.2000', '3. low': '168.2400', '4. close': '168.4500', '5. volume': '37216858'}, '2024-04-05': {'1. open': '169.5900', '2. high': '170.3900', '3. low': '168.9500', '4. close': '169.5800', '5. volume': '41975776'}, '2024-04-04': {'1. open': '170.2900', '2. high': '171.9200', '3. low': '168.8200', '4. close': '168.8200', '5. volume': '53355055'}, '2024-04-03': {'1. open': '168.7900', '2. high': '170.6800', '3. low': '168.5800', '4. close': '169.6500', '5. volume': '45571129'}, '2024-04-02': {'1. open': '169.0800', '2. high': '169.3400', '3. low': '168.2302', '4. close': '168.8400', '5. volume': '49013991'}, '2024-04-01': {'1. open': '171.1900', '2. high': '171.2500', '3. low': '169.4750', '4. close': '170.0300', '5. volume': '43772506'}, '2024-03-28': {'1. open': '171.7500', '2. high': '172.2300', '3. low': '170.5100', '4. close': '171.4800', '5. volume': '65672690'}, '2024-03-27': {'1. open': '170.4100', '2. high': '173.6000', '3. low': '170.1100', '4. close': '173.3100', '5. volume': '60273265'}, '2024-03-26': {'1. open': '170.0000', '2. high': '171.4200', '3. low': '169.5800', '4. close': '169.7100', '5. volume': '57388449'}, '2024-03-25': {'1. open': '170.5650', '2. high': '171.9400', '3. low': '169.4500', '4. close': '170.8500', '5. volume': '54288328'}, '2024-03-22': {'1. open': '171.7600', '2. high': '173.0500', '3. low': '170.0600', '4. close': '172.2800', '5. volume': '71160138'}, '2024-03-21': {'1. open': '177.0500', '2. high': '177.4900', '3. low': '170.8400', '4. close': '171.3700', '5. volume': '106181270'}, '2024-03-20': {'1. open': '175.7200', '2. high': '178.6700', '3. low': '175.0900', '4. close': '178.6700', '5. volume': '53423102'}, '2024-03-19': {'1. open': '174.3400', '2. high': '176.6050', '3. low': '173.0300', '4. close': '176.0800', '5. volume': '55215244'}, '2024-03-18': {'1. open': '175.5700', '2. high': '177.7100', '3. low': '173.5200', '4. close': '173.7200', '5. volume': '75604184'}, '2024-03-15': {'1. open': '171.1700', '2. high': '172.6200', '3. low': '170.2850', '4. close': '172.6200', '5. volume': '121752699'}, '2024-03-14': {'1. open': '172.9100', '2. high': '174.3078', '3. low': '172.0500', '4. close': '173.0000', '5. volume': '72571635'}, '2024-03-13': {'1. open': '172.7700', '2. high': '173.1850', '3. low': '170.7600', '4. close': '171.1300', '5. volume': '51948951'}, '2024-03-12': {'1. open': '173.1500', '2. high': '174.0300', '3. low': '171.0100', '4. close': '173.2300', '5. volume': '59544927'}, '2024-03-11': {'1. open': '172.9400', '2. high': '174.3800', '3. low': '172.0500', '4. close': '172.7500', '5. volume': '58929918'}, '2024-03-08': {'1. open': '169.0000', '2. high': '173.7000', '3. low': '168.9400', '4. close': '170.7300', '5. volume': '76267041'}, '2024-03-07': {'1. open': '169.1500', '2. high': '170.7300', '3. low': '168.4900', '4. close': '169.0000', '5. volume': '71765061'}, '2024-03-06': {'1. open': '171.0600', '2. high': '171.2400', '3. low': '168.6800', '4. close': '169.1200', '5. volume': '68587707'}, '2024-03-05': {'1. open': '170.7600', '2. high': '172.0400', '3. low': '169.6200', '4. close': '170.1200', '5. volume': '95132355'}, '2024-03-04': {'1. open': '176.1500', '2. high': '176.9000', '3. low': '173.7900', '4. close': '175.1000', '5. volume': '81510101'}, '2024-03-01': {'1. open': '179.5500', '2. high': '180.5300', '3. low': '177.3800', '4. close': '179.6600', '5. volume': '73563082'}, '2024-02-29': {'1. open': '181.2700', '2. high': '182.5700', '3. low': '179.5300', '4. close': '180.7500', '5. volume': '136682597'}, '2024-02-28': {'1. open': '182.5100', '2. high': '183.1200', '3. low': '180.1300', '4. close': '181.4200', '5. volume': '48953939'}, '2024-02-27': {'1. open': '181.1000', '2. high': '183.9225', '3. low': '179.5600', '4. close': '182.6300', '5. volume': '54318851'}, '2024-02-26': {'1. open': '182.2400', '2. high': '182.7600', '3. low': '180.6500', '4. close': '181.1600', '5. volume': '40867421'}, '2024-02-23': {'1. open': '185.0100', '2. high': '185.0400', '3. low': '182.2300', '4. close': '182.5200', '5. volume': '45119677'}, '2024-02-22': {'1. open': '183.4800', '2. high': '184.9550', '3. low': '182.4600', '4. close': '184.3700', '5. volume': '52292208'}, '2024-02-21': {'1. open': '181.9400', '2. high': '182.8888', '3. low': '180.6600', '4. close': '182.3200', '5. volume': '41529674'}, '2024-02-20': {'1. open': '181.7900', '2. high': '182.4300', '3. low': '180.0000', '4. close': '181.5600', '5. volume': '53665553'}, '2024-02-16': {'1. open': '183.4200', '2. high': '184.8500', '3. low': '181.6650', '4. close': '182.3100', '5. volume': '49752465'}, '2024-02-15': {'1. open': '183.5500', '2. high': '184.4900', '3. low': '181.3500', '4. close': '183.8600', '5. volume': '65434496'}, '2024-02-14': {'1. open': '185.3200', '2. high': '185.5300', '3. low': '182.4400', '4. close': '184.1500', '5. volume': '54630517'}, '2024-02-13': {'1. open': '185.7700', '2. high': '186.2100', '3. low': '183.5128', '4. close': '185.0400', '5. volume': '56529529'}, '2024-02-12': {'1. open': '188.4150', '2. high': '188.6700', '3. low': '186.7900', '4. close': '187.1500', '5. volume': '41781934'}, '2024-02-09': {'1. open': '188.6500', '2. high': '189.9900', '3. low': '188.0000', '4. close': '188.8500', '5. volume': '45155216'}, '2024-02-08': {'1. open': '189.3850', '2. high': '189.5350', '3. low': '187.3500', '4. close': '188.3200', '5. volume': '40962046'}, '2024-02-07': {'1. open': '190.6400', '2. high': '191.0500', '3. low': '188.6100', '4. close': '189.4100', '5. volume': '53438955'}, '2024-02-06': {'1. open': '186.8600', '2. high': '189.3100', '3. low': '186.7695', '4. close': '189.3000', '5. volume': '43490759'}, '2024-02-05': {'1. open': '188.1500', '2. high': '189.2500', '3. low': '185.8400', '4. close': '187.6800', '5. volume': '69668820'}, '2024-02-02': {'1. open': '179.8600', '2. high': '187.3300', '3. low': '179.2500', '4. close': '185.8500', '5. volume': '102551680'}, '2024-02-01': {'1. open': '183.9850', '2. high': '186.9500', '3. low': '183.8200', '4. close': '186.8600', '5. volume': '64885408'}, '2024-01-31': {'1. open': '187.0400', '2. high': '187.0950', '3. low': '184.3500', '4. close': '184.4000', '5. volume': '55467803'}, '2024-01-30': {'1. open': '190.9400', '2. high': '191.8000', '3. low': '187.4700', '4. close': '188.0400', '5. volume': '55859370'}, '2024-01-29': {'1. open': '192.0100', '2. high': '192.2000', '3. low': '189.5800', '4. close': '191.7300', '5. volume': '47145622'}, '2024-01-26': {'1. open': '194.2700', '2. high': '194.7600', '3. low': '191.9400', '4. close': '192.4200', '5. volume': '44594011'}, '2024-01-25': {'1. open': '195.2200', '2. high': '196.2675', '3. low': '193.1125', '4. close': '194.1700', '5. volume': '54822126'}, '2024-01-24': {'1. open': '195.4200', '2. high': '196.3800', '3. low': '194.3400', '4. close': '194.5000', '5. volume': '53463269'}, '2024-01-23': {'1. open': '195.0200', '2. high': '195.7500', '3. low': '193.8299', '4. close': '195.1800', '5. volume': '42355590'}, '2024-01-22': {'1. open': '192.3000', '2. high': '195.3300', '3. low': '192.2600', '4. close': '193.8900', '5. volume': '60133852'}, '2024-01-19': {'1. open': '189.3300', '2. high': '191.9500', '3. low': '188.8200', '4. close': '191.5600', '5. volume': '68902985'}, '2024-01-18': {'1. open': '186.0900', '2. high': '189.1400', '3. low': '185.8300', '4. close': '188.6300', '5. volume': '78005754'}, '2024-01-17': {'1. open': '181.2700', '2. high': '182.9300', '3. low': '180.3000', '4. close': '182.6800', '5. volume': '47317433'}, '2024-01-16': {'1. open': '182.1600', '2. high': '184.2600', '3. low': '180.9340', '4. close': '183.6300', '5. volume': '65603041'}, '2024-01-12': {'1. open': '186.0600', '2. high': '186.7400', '3. low': '185.1900', '4. close': '185.9200', '5. volume': '40477782'}, '2024-01-11': {'1. open': '186.5400', '2. high': '187.0500', '3. low': '183.6200', '4. close': '185.5900', '5. volume': '49128408'}, '2024-01-10': {'1. open': '184.3500', '2. high': '186.4000', '3. low': '183.9200', '4. close': '186.1900', '5. volume': '46792908'}, '2024-01-09': {'1. open': '183.9200', '2. high': '185.1500', '3. low': '182.7300', '4. close': '185.1400', '5. volume': '42841809'}}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error fetching news: You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-04-30, but you have requested 2024-04-26. You may need to upgrade to a paid plan.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m articles\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m news_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_news\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock market\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2024-04-26\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2024-05-25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews_data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnews_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36mfetch_news\u001b[1;34m(query, from_date, to_date, language)\u001b[0m\n\u001b[0;32m     35\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching news: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown error\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m articles \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m articles\n",
      "\u001b[1;31mValueError\u001b[0m: Error fetching news: You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-04-30, but you have requested 2024-04-26. You may need to upgrade to a paid plan."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Alpha Vantage API\n",
    "ALPHA_VANTAGE_API_KEY = 'EX1OUD6X48E8ZGBL'\n",
    "BASE_URL = 'https://www.alphavantage.co/query'\n",
    "\n",
    "def fetch_stock_data(symbol, outputsize='compact'):\n",
    "    params = {\n",
    "        'function': 'TIME_SERIES_DAILY',\n",
    "        'symbol': symbol,\n",
    "        'apikey': ALPHA_VANTAGE_API_KEY,\n",
    "        'outputsize': outputsize\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "    if \"Error Message\" in data:\n",
    "        raise ValueError(f\"Error fetching data for {symbol}: {data['Error Message']}\")\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "stock_data = fetch_stock_data('AAPL')\n",
    "print(f\"stock_data: {stock_data}\")\n",
    "\n",
    "# News API\n",
    "NEWS_API_KEY = '36612e148e63493d81cde21f9ef75f66'\n",
    "NEWS_URL = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "def fetch_news(query, from_date, to_date, language='en'):\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'from': from_date,\n",
    "        'to': to_date,\n",
    "        'language': language,\n",
    "        'apiKey': NEWS_API_KEY\n",
    "    }\n",
    "    response = requests.get(NEWS_URL, params=params)\n",
    "    data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Error fetching news: {data.get('message', 'Unknown error')}\")\n",
    "    articles = data.get('articles', [])\n",
    "    return articles\n",
    "\n",
    "# Example usage\n",
    "news_data = fetch_news('stock market', '2024-04-26', '2024-05-25')\n",
    "print(f\"news_data: {news_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to prepare the dataset for sentiment analysis\n",
    "def prepare_dataset(articles):\n",
    "    texts = [article['title'] + \" \" + article['content'] for article in articles]\n",
    "    labels = [1 if 'positive' in text else 0 for text in texts]  # Simplified labeling\n",
    "    return Dataset.from_dict({'text': texts, 'label': labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    " #Example of preparing the dataset\n",
    "dataset = prepare_dataset(news_data)\n",
    "print(f\"dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Stock market today: Indexes pop after Nvidia's blowout earnings report US stocks moved higher on Thursday, with investors cheering another blowout earnings report from chip giant Nvidia.\\xa0\\r\\nThe company, whose chips are at the heart of the artificial intelligence boom, beâ€¦ [+2044 chars]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 746.02 examples/s]\n",
      "d:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets: Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 100\n",
      "})\n",
      "train_data: Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 80\n",
      "})\n",
      "eval_data: Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [03:21<05:24, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24218781292438507, 'eval_runtime': 14.8736, 'eval_samples_per_second': 1.345, 'eval_steps_per_second': 0.202, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [06:01<02:24, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07607702165842056, 'eval_runtime': 14.4616, 'eval_samples_per_second': 1.383, 'eval_steps_per_second': 0.207, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [08:32<00:00, 14.66s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [08:47<00:00, 17.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0504133515059948, 'eval_runtime': 14.4605, 'eval_samples_per_second': 1.383, 'eval_steps_per_second': 0.207, 'epoch': 3.0}\n",
      "{'train_runtime': 526.9629, 'train_samples_per_second': 0.455, 'train_steps_per_second': 0.057, 'train_loss': 0.2532611529032389, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "Unable to open proto file: model. Please check if it is a valid proto. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnxruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize_dynamic, QuantType\n\u001b[0;32m     47\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2_quantized.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQuantType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQUInt8\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\onnxruntime\\quantization\\quantize.py:632\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[1;34m(model_input, model_output, op_types_to_quantize, per_channel, reduce_range, weight_type, nodes_to_quantize, nodes_to_exclude, use_external_data_format, extra_options)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m op_types_to_quantize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(op_types_to_quantize) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    627\u001b[0m     op_types_to_quantize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(IntegerOpsRegistry\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    629\u001b[0m model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    630\u001b[0m     save_and_reload_model_with_shape_infer(model_input)\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_input, onnx\u001b[38;5;241m.\u001b[39mModelProto)\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mload_model_with_shape_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m )\n\u001b[0;32m    635\u001b[0m pre_processed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m model_has_pre_process_metadata(model)\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pre_processed:\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\onnxruntime\\quantization\\quant_utils.py:760\u001b[0m, in \u001b[0;36mload_model_with_shape_infer\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_with_shape_infer\u001b[39m(model_path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelProto:\n\u001b[0;32m    759\u001b[0m     inferred_model_path \u001b[38;5;241m=\u001b[39m generate_identified_filename(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inferred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 760\u001b[0m     \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_shapes_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minferred_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m     model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(inferred_model_path\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[0;32m    762\u001b[0m     add_infer_metadata(model)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\onnx\\shape_inference.py:96\u001b[0m, in \u001b[0;36minfer_shapes_path\u001b[1;34m(model_path, output_path, check_type, strict_mode, data_prop)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     95\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m---> 96\u001b[0m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_shapes_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_prop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: Unable to open proto file: model. Please check if it is a valid proto. "
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check the tokenized datasets\n",
    "print(f\"tokenized_datasets: {tokenized_datasets}\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_data = train_test_split['train']\n",
    "eval_data = train_test_split['test']\n",
    "\n",
    "# Check the train and eval datasets\n",
    "print(f\"train_data: {train_data}\")\n",
    "print(f\"eval_data: {eval_data}\")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a dummy input\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Quantize the ONNX model using ONNX Runtime\u001b[39;00m\n\u001b[0;32m     18\u001b[0m quantized_onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2_quantized.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\onnx\\utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[0;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\onnx\\utils.py:1612\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1609\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1610\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1612\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1627\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1628\u001b[0m )\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\onnx\\utils.py:1134\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m   1133\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1134\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\onnx\\utils.py:1010\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m   1006\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     )\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1010\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1011\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m   1012\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\onnx\\utils.py:914\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    912\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    913\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 914\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    923\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\jit\\_trace.py:1310\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   1309\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m-> 1310\u001b[0m outs \u001b[38;5;241m=\u001b[39m ONNXTracedModule(\n\u001b[0;32m   1311\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[0;32m   1312\u001b[0m )(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\jit\\_trace.py:138\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 138\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\jit\\_trace.py:129\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    128\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 129\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    131\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:991\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    989\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 991\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1000\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:803\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    801\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 803\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[0;32m    806\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:126\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03membeddings)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m input_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# isues similar to issue #5664\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Convert the PyTorch model to ONNX\n",
    "onnx_model_path = \"model.onnx\"\n",
    "dummy_input = torch.tensor(tokenizer.encode(\"This is a dummy input\", add_special_tokens=True)).unsqueeze(0)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}, 'output': {0: 'batch_size', 1: 'sequence_length'}}\n",
    ")\n",
    "\n",
    "# Quantize the ONNX model using ONNX Runtime\n",
    "quantized_onnx_model_path = \"gpt2_quantized.onnx\"\n",
    "quantize_dynamic(onnx_model_path, quantized_onnx_model_path, weight_type=QuantType.QUInt8)\n",
    "print(\"Model quantized and saved as ONNX format.\")\n",
    "\n",
    "# Load the quantized ONNX model\n",
    "ort_session = onnxruntime.InferenceSession(quantized_onnx_model_path)\n",
    "\n",
    "def generate_blog_post(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Adjust the input tensor dimensions\n",
    "    max_length = 512\n",
    "    input_ids = torch.nn.functional.pad(input_ids, (0, max_length - input_ids.shape[1]), value=tokenizer.pad_token_id)[:,:max_length]\n",
    "    \n",
    "    # Run inference using the quantized ONNX model\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_ids.cpu().numpy()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    # Ensure the output is cast to integers\n",
    "    ort_outputs_int = ort_outputs[0].astype(int)\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(ort_outputs_int[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"How artificial intelligence is changing the world\"\n",
    "generated_blog_post = generate_blog_post(prompt)\n",
    "print(\"Generated Blog Post:\")\n",
    "print(generated_blog_post)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model quantized and saved as ONNX format.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 6: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     65\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow artificial intelligence is changing the world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 66\u001b[0m generated_blog_post \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_blog_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Blog Post:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_blog_post)\n",
      "Cell \u001b[1;32mIn[62], line 60\u001b[0m, in \u001b[0;36mgenerate_blog_post\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     57\u001b[0m ort_outputs_int \u001b[38;5;241m=\u001b[39m ort_outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Decode the generated text\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mort_outputs_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3836\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3833\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3834\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3837\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3838\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3839\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3841\u001b[0m )\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\tokenization_utils.py:1024\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         current_sub_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[1;32m-> 1024\u001b[0m     sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sub_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[0;32m   1027\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_texts)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py:295\u001b[0m, in \u001b[0;36mGPT2Tokenizer.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_decoder[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 6: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Verify the tokenizer's vocabulary size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Function to validate token IDs\n",
    "def validate_token_ids(input_ids, vocab_size):\n",
    "    if torch.any(input_ids >= vocab_size):\n",
    "        raise ValueError(f\"Token IDs must be within the range [0, {vocab_size - 1}]. Found out-of-range token ID.\")\n",
    "\n",
    "# Create a dummy input and validate it\n",
    "dummy_input = torch.tensor(tokenizer.encode(\"This is a dummy input\", add_special_tokens=True)).unsqueeze(0)\n",
    "validate_token_ids(dummy_input, vocab_size)\n",
    "\n",
    "# Convert the PyTorch model to ONNX\n",
    "onnx_model_path = \"model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}, 'output': {0: 'batch_size', 1: 'sequence_length'}}\n",
    ")\n",
    "\n",
    "# Quantize the ONNX model using ONNX Runtime\n",
    "quantized_onnx_model_path = \"gpt2_quantized.onnx\"\n",
    "quantize_dynamic(onnx_model_path, quantized_onnx_model_path, weight_type=QuantType.QUInt8)\n",
    "print(\"Model quantized and saved as ONNX format.\")\n",
    "\n",
    "# Load the quantized ONNX model\n",
    "ort_session = onnxruntime.InferenceSession(quantized_onnx_model_path)\n",
    "\n",
    "def generate_blog_post(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Validate token IDs\n",
    "    validate_token_ids(input_ids, vocab_size)\n",
    "    \n",
    "    # Adjust the input tensor dimensions\n",
    "    max_length = 512\n",
    "    input_ids = torch.nn.functional.pad(input_ids, (0, max_length - input_ids.shape[1]), value=tokenizer.pad_token_id)[:,:max_length]\n",
    "    \n",
    "    # Run inference using the quantized ONNX model\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_ids.cpu().numpy()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    # Ensure the output is cast to integers\n",
    "    ort_outputs_int = ort_outputs[0].astype(int).flatten()\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(ort_outputs_int, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"How artificial intelligence is changing the world\"\n",
    "generated_blog_post = generate_blog_post(prompt)\n",
    "print(\"Generated Blog Post:\")\n",
    "print(generated_blog_post)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2437, 11666,  4430,   318,  5609,   262,   995]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     23\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow artificial intelligence is changing the world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 24\u001b[0m generated_blog_post \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_blog_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Blog Post:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_blog_post)\n",
      "Cell \u001b[1;32mIn[65], line 17\u001b[0m, in \u001b[0;36mgenerate_blog_post\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     14\u001b[0m ort_outputs_int \u001b[38;5;241m=\u001b[39m ort_outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Decode the generated text, handling None values\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m decoded_tokens \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode([token_id], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m ort_outputs_int]\n\u001b[0;32m     18\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(decoded_tokens)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "Cell \u001b[1;32mIn[65], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m ort_outputs_int \u001b[38;5;241m=\u001b[39m ort_outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Decode the generated text, handling None values\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m decoded_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m ort_outputs_int]\n\u001b[0;32m     18\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(decoded_tokens)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3836\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3833\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3834\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3837\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3838\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3839\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3841\u001b[0m )\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\tokenization_utils.py:1024\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         current_sub_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[1;32m-> 1024\u001b[0m     sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sub_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[0;32m   1027\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_texts)\n",
      "File \u001b[1;32md:\\Data science\\my_projects\\blog_generation\\blog\\lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py:295\u001b[0m, in \u001b[0;36mGPT2Tokenizer.convert_tokens_to_string\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_decoder[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "def generate_blog_post(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Adjust the input tensor dimensions\n",
    "    max_length = 512\n",
    "    input_ids = torch.nn.functional.pad(input_ids, (0, max_length - input_ids.shape[1]), value=tokenizer.pad_token_id)[:,:max_length]\n",
    "    \n",
    "    # Run inference using the quantized ONNX model\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_ids.cpu().numpy()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    # Ensure the output is cast to integers and flatten the array\n",
    "    ort_outputs_int = ort_outputs[0].astype(int).flatten()\n",
    "    \n",
    "    # Decode the generated text, handling None values\n",
    "    decoded_tokens = [tokenizer.decode([token_id], skip_special_tokens=True) if token_id is not None else \"\" for token_id in ort_outputs_int]\n",
    "    generated_text = \"\".join(decoded_tokens)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"How artificial intelligence is changing the world\"\n",
    "generated_blog_post = generate_blog_post(prompt)\n",
    "print(\"Generated Blog Post:\")\n",
    "print(generated_blog_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import re\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "#     text = re.sub(r'\\W', ' ', text)   # Remove non-word characters\n",
    "#     text = text.lower()               # Convert to lowercase\n",
    "#     tokens = word_tokenize(text)      # Tokenize text\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# # Example usage\n",
    "# sample_text = \"The stock market is booming in 2023!\"\n",
    "# cleaned_text = preprocess_text(sample_text)\n",
    "# print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "# model_name = \"gpt2\"\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Tokenize and preprocess data\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# # Prepare dataset\n",
    "# train_data = pd.DataFrame(news_data)  # Assuming news_data is a list of news articles\n",
    "# train_data\n",
    "# train_data['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['text'] = train_data['description'].apply(preprocess_text)\n",
    "# train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Fine-tune model\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     num_train_epochs=3,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_data,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # Quantize model using ONNX Runtime\n",
    "# from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# onnx_model_path = \"gpt2_quantized.onnx\"\n",
    "# model.save_pretrained(\"./model\")\n",
    "# tokenizer.save_pretrained(\"./model\")\n",
    "# quantize_dynamic(\"./model\", onnx_model_path, weight_type=QuantType.QUInt8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
